{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b4a0d60",
   "metadata": {},
   "source": [
    "# VectorStore-backed Retriever\n",
    "\n",
    "- Author: [Erika Park](https://www.linkedin.com/in/yeonseo-park-094193198/)\n",
    "- Designer: [Erika Park](https://www.linkedin.com/in/yeonseo-park-094193198/)\n",
    "- Peer Review: \n",
    "- Proofread:\n",
    "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/01-Basic/05-Using-OpenAIAPI-MultiModal.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/01-Basic/05-Using-OpenAIAPI-MultiModal.ipynb)\n",
    "\n",
    "## Overview\n",
    "This tutorial provides a comprehensive guide to building and optimizing a **VectorStore-backed retriever** using LangChain. It covers the foundational steps of creating a vector store with FAISS(Facebook AI Similarity Search) and explores advanced retrieval strategies for improving search accuracy and efficiency.\n",
    "\n",
    "A **VectorStore-backed retriever** is a document retrieval system that leverages a vector store to search for documents based on their vector representations. This approach enables efficient similarity-based search for handling unstructured data.\n",
    "\n",
    "\n",
    "### RAG (Retrieval-Augmented Generation) Workflow\n",
    "<img src=\"./assets/01-vectorstore-retriever-rag-flow.png\" alt=\"rag-flow\" width=\"1000\">\n",
    "\n",
    "The diagram above illustrates the  **document search and response generation** workflow within a RAG system. \n",
    "\n",
    "The steps include:\n",
    "\n",
    "1. Document Loading: Importing raw documents.  \n",
    "2. Text Chunking: Splitting text into manageable chunks.  \n",
    "3. Vector Embedding: Converting the text into numerical vectors using an embedding model.  \n",
    "4. Store in Vector Database: Storing the generated embeddings in a vector database for efficient retrieval.\n",
    "\n",
    "During the query phase:\n",
    "- Steps: User Query ‚Üí Embedding ‚Üí Search in VectorStore ‚Üí Relevant Chunks Retrieved ‚Üí LLM Generates Response\n",
    "- The user's query is transformed into an embedding vector using an embedding model.\n",
    "- This query embedding is compared against stored document vectors within the vector database to **retrieve the most relevant results**.\n",
    "- The retrieved chunks are passed to a Large Language Model (LLM), which generates a final response based on the retrieved information.\n",
    "\n",
    "This tutorial aims to explore and optimize the VectorStore ‚Üí Relevant Chunks Retrieved ‚Üí LLM Generates Response stages. It will cover advanced retrieval techniques to improve the accuracy and relevance of the responses.\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [Overview](#overview)\n",
    "- [Environment Setup](#environment-setup)\n",
    "- [Initializing and Using VectorStoreRetriever](#initializing-and-using-vectorstoreretriever)\n",
    "- [Dynamic Configuration (Using ConfigurableField)](#dynamic-configuration-using-configurablefield)\n",
    "- [Using Separate Query & Passage Embedding Models](#using-separate-query--passage-embedding-models)\n",
    "\n",
    "### References\n",
    "\n",
    "- [How to use a vectorstore as a retriever](https://python.langchain.com/docs/how_to/vectorstore_retriever/)\n",
    "- [Maximum Marginal Relevance (MMR)](https://community.fullstackretrieval.com/retrieval-methods/maximum-marginal-relevance)\n",
    "- [Upstage-Embeddings](https://console.upstage.ai/docs/capabilities/embeddings)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c6af75",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
    "\n",
    "**[Note]**\n",
    "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions, and utilities for tutorials. \n",
    "- You can checkout out the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9a9e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain-opentutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3c34330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "from langchain_opentutorial import package\n",
    "\n",
    "package.install(\n",
    "    [\n",
    "        \"langchain_opentutorial\",\n",
    "        \"langchain_openai\",\n",
    "        \"langchain_community\",\n",
    "        \"langchain_text_splitters\",\n",
    "        \"langchain_core\",\n",
    "        \"langchain_upstage\",\n",
    "        \"faiss-cpu\"\n",
    "    ],\n",
    "    verbose=False,\n",
    "    upgrade=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2bf9b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables have been set successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables\n",
    "from langchain_opentutorial import set_env\n",
    "\n",
    "set_env(\n",
    "    {\n",
    "        # \"OPENAI_API_KEY\": \"\",\n",
    "        # \"LANGCHAIN_API_KEY\": \"\",\n",
    "        # \"UPSTAGE_API_KEY\": \"\",\n",
    "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
    "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
    "        \"LANGCHAIN_PROJECT\": \"VectorStore Retriever\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c24f41",
   "metadata": {},
   "source": [
    "You can alternatively set API keys such as `OPENAI_API_KEY` in a `.env` file and load them.\n",
    "\n",
    "[Note] This is not necessary if you've already set the required API keys in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfaffe52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration file to manage the API KEY as an environment variable\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API KEY information\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e66b00",
   "metadata": {},
   "source": [
    "## Initializing and Using VectorStoreRetriever\n",
    "\n",
    "This section demonstrates how to load documents using OpenAI embeddings and create a vector database using FAISS.\n",
    "\n",
    "- The example below showcases how to use OpenAI embeddings for document loading and FAISS for vector database creation.\n",
    "- Once the vector database is created, it can be loaded and queried using retrieval methods such as **Similarity Search** and **Maximal Marginal Relevance (MMR)** to search for relevant text within the vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc586a1",
   "metadata": {},
   "source": [
    "üìå **Creating a Vector Store (Using FAISS)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9b98ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 351, which is longer than the specified 300\n",
      "Created a chunk of size 343, which is longer than the specified 300\n",
      "Created a chunk of size 307, which is longer than the specified 300\n",
      "Created a chunk of size 316, which is longer than the specified 300\n",
      "Created a chunk of size 341, which is longer than the specified 300\n",
      "Created a chunk of size 321, which is longer than the specified 300\n",
      "Created a chunk of size 303, which is longer than the specified 300\n",
      "Created a chunk of size 325, which is longer than the specified 300\n",
      "Created a chunk of size 315, which is longer than the specified 300\n",
      "Created a chunk of size 304, which is longer than the specified 300\n",
      "Created a chunk of size 385, which is longer than the specified 300\n",
      "Created a chunk of size 349, which is longer than the specified 300\n",
      "Created a chunk of size 376, which is longer than the specified 300\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Load the file using TextLoader\n",
    "loader = TextLoader(\"./data/01-vectorstore-retriever-appendix-keywords.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "# split the text into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "split_docs = text_splitter.split_documents(documents) # Split into smaller chunks\n",
    "\n",
    "# Initialize the OpenAI embedding model\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Create a FAISS vector database\n",
    "db = FAISS.from_documents(split_docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fbd962",
   "metadata": {},
   "source": [
    "üìå **1. Initializing and Using VectorStoreRetriever (`as_retriever` )**\n",
    "\n",
    "The `as_retriever` method allows you to convert a vector database into a retriever, enabling efficient document search and retrieval from the vector store.\n",
    "\n",
    "**How It Works**:\n",
    "* The `as_retriever()` method transforms a vector store (like FAISS) into a retriever object, making it compatible with LangChain's retrieval workflows.\n",
    "* This retriever can then be directly used with RAG pipelines or combined with Large Language Models (LLMs) for building intelligent search systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05d21318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Retriever Creation (Similarity Search)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d51d7bc",
   "metadata": {},
   "source": [
    "**Advanced Retriever Configuration**\n",
    "\n",
    "The `as_retriever` method allows you to configure advanced retrieval strategies, such as **similarity search**, **MMR (Maximal Marginal Relevance)**, and **similarity score threshold-based filtering**.\n",
    "\n",
    "\n",
    "**Parameters:**\n",
    "\n",
    "- `**kwargs` : Keyword arguments passed to the retrieval function:\n",
    "   - `search_type` : Specifies the search method.\n",
    "     - `\"similarity\"` : Returns the most relevant documents based on cosine similarity.\n",
    "     - `\"mmr\"` : Utilizes the Maximal Marginal Relevance algorithm, balancing **relevance** and **diversity**.\n",
    "     - `\"similarity_score_threshold\"` : Returns documents with a similarity score above a specified threshold.\n",
    "   - `search_kwargs` : Additional search options for fine-tuning results:\n",
    "     - `k` : Number of documents to return (default: `4` ).\n",
    "     - `score_threshold` : Minimum similarity score for the `\"similarity_score_threshold\"` search type (e.g., `0.8` ).\n",
    "     - `fetch_k` : Number of documents initially retrieved during an MMR search (default: `20` ).\n",
    "     - `lambda_mult` : Controls diversity in MMR results (`0` = maximum diversity, `1` = maximum relevance, default: `0.5` ).\n",
    "     - `filter` : Metadata filtering for selective document retrieval.\n",
    "\n",
    "\n",
    " **Return Value:**\n",
    "\n",
    "- `VectorStoreRetriever`: An initialized retriever object that can be directly queried for document search tasks.\n",
    "\n",
    "\n",
    "**Notes:**\n",
    "- Supports multiple search strategies (`similarity` , `MMR` , `similarity_score_threshold` ).\n",
    "- MMR improves result diversity while preserving relevance by reducing redundancy in results.\n",
    "- Metadata filtering enables selective document retrieval based on document properties.\n",
    "- The `tags` parameter can be used to label retrievers for better organization and easier identification.\n",
    "\n",
    " **Cautions:**\n",
    "- Diversity Control with MMR:\n",
    "  - Adjust both `fetch_k` (number of documents initially retrieved) and `lambda_mult` (diversity control factor) carefully for optimal balance.\n",
    "  - `lambda_mult`\n",
    "    - Lower values (< 0.5) ‚Üí Prioritize diversity.\n",
    "    - Higher values (> 0.5) ‚Üí Prioritize relevance.\n",
    "  - set `fetch_k` higher than `k` for effective diversity control.\n",
    "- Threshold Settings: \n",
    "  - Using a high `score_threshold` (e.g., 0.95) can lead to zero results.\n",
    "- Metadata Filtering: \n",
    "  - Ensure the metadata structure is well-defined before applying filters.\n",
    "- Balanced Configuration:\n",
    "  - Maintain a proper balance between `search_type` and `search_kwargs` settings for optimal retrieval performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c038da2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Search\n",
      "VectorStore\n",
      "\n",
      "Definition: A vector store is a system for storing data in vector format, often used for search, classification, and data analysis tasks.\n",
      "Example: Storing word embeddings in a database for fast retrieval of similar words.\n",
      "Related Keywords: Embedding, Database, Vectorization\n",
      "Definition: Semantic search is a method of retrieving results based on the meaning of the user's query, going beyond simple keyword matching.\n",
      "Example: If a user searches for \"solar system planets,\" the search returns information about related planets like Jupiter and Mars.\n",
      "Related Keywords: Natural Language Processing, Search Algorithms, Data Mining\n",
      "Definition: Keyword search is the process of finding information based on specific keywords entered by the user. It is commonly used in search engines and database systems as a fundamental search method.\n",
      "Example: If a user searches for \"coffee shop in Seoul,\" the search engine returns a list of related coffee shops.\n",
      "Related Keywords: Search Engine, Data Retrieval, Information Search\n",
      "Definition: FAISS is a high-speed similarity search library developed by Facebook, designed for efficient vector searches in large datasets.\n",
      "Example: Searching for similar images in a dataset of millions using FAISS.\n",
      "Related Keywords: Vector Search, Machine Learning, Database Optimization\n"
     ]
    }
   ],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\", \n",
    "    search_kwargs={\n",
    "        \"k\": 5,  # Return the top 5 most relevant documents\n",
    "        \"score_threshold\": 0.7  # Only return documents with a similarity score of 0.7 or higher\n",
    "    }\n",
    ")\n",
    "# Perform the search\n",
    "query = \"Explain the concept of vector search.\"\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "# Display search results\n",
    "for doc in results:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d741849",
   "metadata": {},
   "source": [
    "### Retriever's `invoke()` Method\n",
    "\n",
    "The `invoke()` method is the primary entry point for interacting with a Retriever. It is used to search and retrieve relevant documents based on a given query.\n",
    "\n",
    "**How It Works** :\n",
    "1. Query Submission: A user query is provided as input.\n",
    "2. Embedding Generation: The query is converted into a vector representation (if necessary).\n",
    "3. Search Process: The retriever searches the vector database using the specified search strategy (similarity, MMR, etc.).\n",
    "4. Results Return: The method returns a list of relevant document chunks.\n",
    "\n",
    " **Parameters:**\n",
    "- `input` (Required):\n",
    "   - The query string provided by the user.\n",
    "   - The query is converted into a vector and compared with stored document vectors for similarity-based retrieval.\n",
    "\n",
    "- `config` (Optional):\n",
    "   - Allows for fine-grained control over the retrieval process.\n",
    "   - Can be used to specify **tags, metadata insertion, and search strategies**.\n",
    "\n",
    "- `**kwargs` (Optional):\n",
    "   - Enables direct passing of `search_kwargs` for advanced configuration.\n",
    "   - Example options include:\n",
    "     - `k` : Number of documents to return.\n",
    "     - `score_threshold` : Minimum similarity score for a document to be included.\n",
    "     - `fetch_k` : Number of documents initially retrieved in MMR searches.\n",
    "\n",
    "\n",
    " **Return Value:**\n",
    "- `List[Document]`:\n",
    "   - Returns a list of document objects containing the retrieved text and metadata.\n",
    "   - Each document object includes:\n",
    "     - `page_content` : The main content of the document.\n",
    "     - `metadata` : Associated metadata with the document (e.g., source, tags).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51834bb6",
   "metadata": {},
   "source": [
    "**Usage Example 1: Basic Usage (Synchronous Search)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "257d2ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding\n",
      "=========================================================\n",
      "Definition: Embedding is the process of converting text data, such as words or sentences, into continuous low-dimensional vectors. This allows computers to understand and process text.\n",
      "Example: The word \"apple\" can be represented as a vector like [0.65, -0.23, 0.17].\n",
      "Related Keywords: Natural Language Processing, Vectorization, Deep Learning\n",
      "=========================================================\n",
      "Semantic Search\n",
      "=========================================================\n",
      "Deep Learning\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.invoke(\"What is an embedding?\")\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5d5ba5",
   "metadata": {},
   "source": [
    "**Usage Example 2: Search with Options** ( `search_kwargs` )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "643a6491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorStore\n",
      "\n",
      "Definition: A vector store is a system for storing data in vector format, often used for search, classification, and data analysis tasks.\n",
      "Example: Storing word embeddings in a database for fast retrieval of similar words.\n",
      "Related Keywords: Embedding, Database, Vectorization\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "# search options: top 5 results with a similarity score ‚â• 0.7\n",
    "docs = retriever.invoke(\n",
    "    \"What is a vector database?\",\n",
    "    search_kwargs={\"k\": 5, \"score_threshold\": 0.7}\n",
    ")\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e360a59",
   "metadata": {},
   "source": [
    "**Usage Example 3: Using** `config` **and** `**kwargs` **(Advanced Configuration)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d58154d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç [Search Result 1]\n",
      "üìÑ Document Content: Definition: A DataFrame is a tabular data structure with rows and columns, commonly used for data analysis and manipulation.\n",
      "Example: Pandas DataFrame can store data like an Excel sheet and perform operations like filtering and grouping.\n",
      "Related Keywords: Data Analysis, Pandas, Data Manipulation\n",
      "üóÇÔ∏è Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "\n",
      "üîç [Search Result 2]\n",
      "üìÑ Document Content: Schema\n",
      "\n",
      "Definition: A schema defines the structure of a database or file, describing how data is stored and organized.\n",
      "Example: A database schema can specify table columns, data types, and constraints.\n",
      "Related Keywords: Database, Data Modeling, Data Management\n",
      "\n",
      "DataFrame\n",
      "üóÇÔ∏è Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "\n",
      "üîç [Search Result 3]\n",
      "üìÑ Document Content: Pandas\n",
      "\n",
      "Definition: Pandas is a Python library for data analysis and manipulation, offering tools for working with structured data.\n",
      "Example: Pandas can read CSV files, clean data, and perform statistical analysis.\n",
      "Related Keywords: Data Analysis, Python, Data Manipulation\n",
      "üóÇÔ∏è Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "\n",
      "üîç [Search Result 4]\n",
      "üìÑ Document Content: Data Mining\n",
      "üóÇÔ∏è Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "# Create a RunnableConfig with tags and metadata\n",
    "config = RunnableConfig(\n",
    "    tags=[\"retrieval\", \"faq\"],  ## Adding tags for query categorization\n",
    "    metadata={\"project\": \"vectorstore-tutorial\"}  # Project-specific metadata for traceability\n",
    ")\n",
    "# Perform a query using advanced configuration settings\n",
    "docs = retriever.invoke(\n",
    "    input=\"What is a DataFrame?\", \n",
    "    config=config,  # Applying the config with tags and metadata\n",
    "    search_kwargs={\n",
    "        \"k\": 3,                   \n",
    "        \"score_threshold\": 0.8   \n",
    "    }\n",
    ")\n",
    "#  Display the search results\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(f\"\\nüîç [Search Result {idx + 1}]\")\n",
    "    print(\"üìÑ Document Content:\", doc.page_content)\n",
    "    print(\"üóÇÔ∏è Metadata:\", doc.metadata)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab94c2c",
   "metadata": {},
   "source": [
    "### Max Marginal Relevance (MMR)\n",
    "\n",
    "The **Maximal Marginal Relevance (MMR)** search method is a document retrieval algorithm designed to reduce redundancy by balancing relevance and diversity when returning results.\n",
    "\n",
    "**How MMR Works:**\n",
    "Unlike basic similarity-based searches that return the most relevant documents based solely on similarity scores, MMR considers two critical factors:\n",
    "1. Relevance: Measures how closely the document matches the user's query.\n",
    "2. Diversity: Ensures the retrieved documents are distinct from each other to avoid repetitive results.\n",
    "\n",
    " **Key Parameters:**\n",
    "- `search_type=\"mmr\"`: Activates the MMR retrieval strategy.  \n",
    "- `k`: The number of documents returned after applying diversity filtering(default: `4`).  \n",
    "- `fetch_k`: Number of documents initially retrieved before applying diversity filtering (default: `20`).  \n",
    "- `lambda_mult`: Diversity control factor (`0 = max diversity` , `1 = max relevance` , default: `0.5`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8144a926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé [Query]: What is an embedding?\n",
      "\n",
      "üìÑ [Document 1]\n",
      "üìñ Document Content: Embedding\n",
      "üóÇÔ∏è Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "üìÑ [Document 2]\n",
      "üìñ Document Content: Definition: Embedding is the process of converting text data, such as words or sentences, into continuous low-dimensional vectors. This allows computers to understand and process text.\n",
      "Example: The word \"apple\" can be represented as a vector like [0.65, -0.23, 0.17].\n",
      "Related Keywords: Natural Language Processing, Vectorization, Deep Learning\n",
      "üóÇÔ∏è Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "üìÑ [Document 3]\n",
      "üìñ Document Content: TF-IDF (Term Frequency-Inverse Document Frequency)\n",
      "üóÇÔ∏è Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# MMR Retriever Configuration (Balancing Relevance and Diversity)\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\", \n",
    "    search_kwargs={\n",
    "        \"k\": 3,                \n",
    "        \"fetch_k\": 10,           \n",
    "        \"lambda_mult\": 0.6  # Balancing Similarity and Diversity (0.6: Slight Emphasis on Diversity)\n",
    "    }\n",
    ")\n",
    "\n",
    "query = \"What is an embedding?\"\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "#  Display the search results\n",
    "print(f\"\\nüîé [Query]: {query}\\n\")\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(f\"üìÑ [Document {idx + 1}]\")\n",
    "    print(\"üìñ Document Content:\", doc.page_content)\n",
    "    print(\"üóÇÔ∏è Metadata:\", doc.metadata)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902babfe",
   "metadata": {},
   "source": [
    "### Similarity Score Threshold Search\n",
    "\n",
    "**Similarity Score Threshold Search** is a retrieval method where only documents exceeding a predefined similarity score are returned. This approach helps filter out low-relevance results, ensuring that the returned documents are highly relevant to the query.\n",
    "\n",
    "**Key Features:**\n",
    "- Relevance Filtering: Returns only documents with a similarity score above the specified threshold.\n",
    "- Configurable Precision: The threshold is adjustable using the `score_threshold` parameter.\n",
    "- Search Type Activation: Enabled by setting `search_type=\"similarity_score_threshold\"` .\n",
    "\n",
    "This search method is ideal for tasks requiring **highly precise** results, such as fact-checking or answering technical queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6509f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé [Query]: What is Word2Vec?\n",
      "\n",
      "üìÑ [Document 1]\n",
      "üìñ Document Content: Word2Vec\n",
      "\n",
      "Definition: Word2Vec is a technique in NLP that maps words into a vector space, representing their semantic relationships based on context.\n",
      "Example: In Word2Vec, \"king\" and \"queen\" would be represented by vectors close to each other.\n",
      "Related Keywords: NLP, Embeddings, Semantic Similarity\n",
      "üóÇÔ∏è Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "üìÑ [Document 2]\n",
      "üìñ Document Content: Definition: Embedding is the process of converting text data, such as words or sentences, into continuous low-dimensional vectors. This allows computers to understand and process text.\n",
      "Example: The word \"apple\" can be represented as a vector like [0.65, -0.23, 0.17].\n",
      "Related Keywords: Natural Language Processing, Vectorization, Deep Learning\n",
      "üóÇÔ∏è Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "üìÑ [Document 3]\n",
      "üìñ Document Content: TF-IDF (Term Frequency-Inverse Document Frequency)\n",
      "üóÇÔ∏è Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "üìÑ [Document 4]\n",
      "üìñ Document Content: Tokenizer\n",
      "üóÇÔ∏è Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "üìÑ [Document 5]\n",
      "üìñ Document Content: Semantic Search\n",
      "üóÇÔ∏è Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Retriever Configuration (Similarity Score Threshold Search)\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",  \n",
    "    search_kwargs={\n",
    "        \"score_threshold\": 0.6,  \n",
    "        \"k\": 5                \n",
    "    }\n",
    ")\n",
    "# Execute the query\n",
    "query = \"What is Word2Vec?\"\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "# Display the search results \n",
    "print(f\"\\nüîé [Query]: {query}\\n\")\n",
    "if docs:\n",
    "    for idx, doc in enumerate(docs):\n",
    "        print(f\"üìÑ [Document {idx + 1}]\")\n",
    "        print(\"üìñ Document Content:\", doc.page_content)\n",
    "        print(\"üóÇÔ∏è Metadata:\", doc.metadata)\n",
    "        print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No relevant documents found. Try lowering the similarity score threshold.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b16c14f",
   "metadata": {},
   "source": [
    "### Configuring `top_k` (Adjusting the Number of Returned Documents)\n",
    "\n",
    "- The parameter `k` specifies the number of documents returned during a vector search. It determines how many of the **top-ranked** documents (based on similarity score) will be retrieved from the vector database.\n",
    "\n",
    "- The number of documents retrieved can be adjusted by setting the `k` value within the `search_kwargs`.  \n",
    "- For example, setting `k=1` will return only the **top 1 most relevant document** based on similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "081e0134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé [Query]: What is an embedding?\n",
      "\n",
      "üìÑ [Document 1]\n",
      "üìñ Document Content: Embedding\n",
      "üóÇÔ∏è Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Retriever Configuration (Return Only the Top 1 Document)\n",
    "retriever = db.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 1  # Return only the top 1 most relevant document\n",
    "    }\n",
    ")\n",
    "\n",
    "query = \"What is an embedding?\"\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "#  Display the search results \n",
    "print(f\"\\nüîé [Query]: {query}\\n\")\n",
    "if docs:\n",
    "    for idx, doc in enumerate(docs):\n",
    "        print(f\"üìÑ [Document {idx + 1}]\")\n",
    "        print(\"üìñ Document Content:\", doc.page_content)\n",
    "        print(\"üóÇÔ∏è Metadata:\", doc.metadata)\n",
    "        print(\"=\" * 60)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No relevant documents found. Try increasing the `k` value.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1168ca53",
   "metadata": {},
   "source": [
    "## Dynamic Configuration (Using `ConfigurableField` )\n",
    "\n",
    "The `ConfigurableField` feature in LangChain allows for **dynamic adjustment** of search configurations, providing flexibility during query execution.\n",
    "\n",
    "**Key Features:**\n",
    "- Runtime Search Configuration: Adjust search settings without modifying the core retriever setup.\n",
    "- Enhanced Traceability: Assign unique identifiers, names, and descriptions to each parameter for improved readability and debugging.\n",
    "- Flexible Control with `config`: Search configurations can be passed dynamically using the `config` parameter as a dictionary.\n",
    "\n",
    "\n",
    "**Use Cases:**\n",
    "- Switching Search Strategies: Dynamically adjust the search type (e.g., `\"similarity\"`, `\"mmr\"` ).\n",
    "- Real-Time Parameter Adjustments: Modify search parameters like `k` , `score_threshold` , and `fetch_k` during query execution.\n",
    "- Experimentation: Easily test different search strategies and parameter combinations without rewriting code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc25029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableField \n",
    "\n",
    "# Retriever Configuration Using ConfigurableField\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 1}).configurable_fields(\n",
    "    search_type=ConfigurableField(\n",
    "        id=\"search_type\", \n",
    "        name=\"Search Type\",  # Name for the search strategy\n",
    "        description=\"The search type to use\",  # Description of the search strategy\n",
    "    ),\n",
    "    search_kwargs=ConfigurableField(\n",
    "        id=\"search_kwargs\",  \n",
    "        name=\"Search Kwargs\",  # Name for the search parameters\n",
    "        description=\"The search kwargs to use\",  # Description of the search parameters\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721b750c",
   "metadata": {},
   "source": [
    "The following examples demonstrate how to apply dynamic search settings using `ConfigurableField` in LangChain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48f53c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé [Search Results - Basic Configuration (Top 3 Documents)]\n",
      "üìÑ [Document 1]\n",
      "Embedding\n",
      "============================================================\n",
      "üìÑ [Document 2]\n",
      "Definition: Embedding is the process of converting text data, such as words or sentences, into continuous low-dimensional vectors. This allows computers to understand and process text.\n",
      "Example: The word \"apple\" can be represented as a vector like [0.65, -0.23, 0.17].\n",
      "Related Keywords: Natural Language Processing, Vectorization, Deep Learning\n",
      "============================================================\n",
      "üìÑ [Document 3]\n",
      "Semantic Search\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Search Configuration 1: Basic Search (Top 3 Documents)\n",
    "\n",
    "config_1 = {\"configurable\": {\"search_kwargs\": {\"k\": 3}}}\n",
    "\n",
    "# Execute the query\n",
    "docs = retriever.invoke(\"What is an embedding?\", config=config_1)\n",
    "\n",
    "# Display the search results\n",
    "print(\"\\nüîé [Search Results - Basic Configuration (Top 3 Documents)]\")\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(f\"üìÑ [Document {idx + 1}]\")\n",
    "    print(doc.page_content)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "951851c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé [Search Results - Similarity Score Threshold ‚â• 0.8]\n",
      "üìÑ [Document 1]\n",
      "Word2Vec\n",
      "\n",
      "Definition: Word2Vec is a technique in NLP that maps words into a vector space, representing their semantic relationships based on context.\n",
      "Example: In Word2Vec, \"king\" and \"queen\" would be represented by vectors close to each other.\n",
      "Related Keywords: NLP, Embeddings, Semantic Similarity\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Search Configuration 2: Similarity Score Threshold (‚â• 0.8)\n",
    "\n",
    "config_2 = {\n",
    "    \"configurable\": {\n",
    "        \"search_type\": \"similarity_score_threshold\",\n",
    "        \"search_kwargs\": {\n",
    "            \"score_threshold\": 0.8,  # Only return documents with a similarity score of 0.8 or higher\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "# Execute the query\n",
    "docs = retriever.invoke(\"What is Word2Vec?\", config=config_2)\n",
    "\n",
    "# Display the search results\n",
    "print(\"\\nüîé [Search Results - Similarity Score Threshold ‚â• 0.8]\")\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(f\"üìÑ [Document {idx + 1}]\")\n",
    "    print(doc.page_content)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02e6e23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé [Search Results - MMR (Diversity and Relevance Balanced)]\n",
      "üìÑ [Document 1]\n",
      "Word2Vec\n",
      "\n",
      "Definition: Word2Vec is a technique in NLP that maps words into a vector space, representing their semantic relationships based on context.\n",
      "Example: In Word2Vec, \"king\" and \"queen\" would be represented by vectors close to each other.\n",
      "Related Keywords: NLP, Embeddings, Semantic Similarity\n",
      "============================================================\n",
      "üìÑ [Document 2]\n",
      "Tokenizer\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Search Configuration 3: MMR Search (Diversity and Relevance Balanced)\n",
    "\n",
    "config_3 = {\n",
    "    \"configurable\": {\n",
    "        \"search_type\": \"mmr\",\n",
    "        \"search_kwargs\": {\n",
    "            \"k\": 2,            # Return the top 2 most diverse and relevant documents\n",
    "            \"fetch_k\": 10,     # Initially fetch the top 10 documents before filtering for diversity\n",
    "            \"lambda_mult\": 0.6 # Balance factor: 0.6 (0 = maximum diversity, 1 = maximum relevance)\n",
    "        },\n",
    "    }\n",
    "}\n",
    "# Execute the query using MMR search\n",
    "docs = retriever.invoke(\"What is Word2Vec?\", config=config_3)\n",
    "\n",
    "#  Display the search results\n",
    "print(\"\\nüîé [Search Results - MMR (Diversity and Relevance Balanced)]\")\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(f\"üìÑ [Document {idx + 1}]\")\n",
    "    print(doc.page_content)\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbddf405",
   "metadata": {},
   "source": [
    "## Using Separate Query & Passage Embedding Models\n",
    "\n",
    "By default, a retriever uses the **same embedding model** for both queries and documents. However, certain scenarios can benefit from using different models tailored to the specific needs of queries and documents.\n",
    "\n",
    "### Why Use Separate Embedding Models?\n",
    "Using different models for queries and documents can improve retrieval accuracy and search relevance by optimizing each model for its intended purpose:\n",
    "- Query Embedding Model: Fine-tuned for understanding short and concise search queries.\n",
    "- Document (Passage) Embedding Model: Optimized for longer text spans with richer context.\n",
    "  \n",
    "For instance, **Upstage Embeddings** provides the capability to use distinct models for:  \n",
    "- Query Embeddings (`solar-embedding-1-large-query`)  \n",
    "- Document (Passage) Embeddings (`solar-embedding-1-large-passage`)  \n",
    "\n",
    "In such cases, the query is embedded using the query embedding model, while the documents are embedded using the document embedding model. \n",
    "\n",
    "‚úÖ **How to Issue an Upstage API Key**  \n",
    "- Sign Up & Log In: \n",
    "   - Visit [Upstage](https://upstage.ai/) and log in (sign up if you don't have an account).  \n",
    "\n",
    "- Open API Key Page:\n",
    "   - Go to the menu bar, select \"Dashboards\", then navigate to \"API Keys\".\n",
    "\n",
    "- Generate API Key:  \n",
    "   - Click **\"Create new key\"** ‚Üí Enter name your key (e.g., `LangChain-Tutorial`) \n",
    "\n",
    "- Copy & Store Safely:  \n",
    "   - Copy the generated key and keep it secure.  \n",
    "\n",
    "<img src=\"./assets/01-vectorstore-retriever-get-upstage-api-key.png\" alt=\"Description\" width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b51093b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 351, which is longer than the specified 300\n",
      "Created a chunk of size 343, which is longer than the specified 300\n",
      "Created a chunk of size 307, which is longer than the specified 300\n",
      "Created a chunk of size 316, which is longer than the specified 300\n",
      "Created a chunk of size 341, which is longer than the specified 300\n",
      "Created a chunk of size 321, which is longer than the specified 300\n",
      "Created a chunk of size 303, which is longer than the specified 300\n",
      "Created a chunk of size 325, which is longer than the specified 300\n",
      "Created a chunk of size 315, which is longer than the specified 300\n",
      "Created a chunk of size 304, which is longer than the specified 300\n",
      "Created a chunk of size 385, which is longer than the specified 300\n",
      "Created a chunk of size 349, which is longer than the specified 300\n",
      "Created a chunk of size 376, which is longer than the specified 300\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "\n",
    "# ‚úÖ 1. Data Loading and Document Splitting\n",
    "loader = TextLoader(\"./data/01-vectorstore-retriever-appendix-keywords.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the loaded documents into text chunks \n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# ‚úÖ 2. Document Embedding\n",
    "doc_embedder = UpstageEmbeddings(model=\"solar-embedding-1-large-passage\")\n",
    "\n",
    "# ‚úÖ 3. Create a Vector Database\n",
    "db = FAISS.from_documents(split_docs, doc_embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424f69e7",
   "metadata": {},
   "source": [
    "The following example demonstrates the process of generating an Upstage embedding for a query, converting the query sentence into a vector, and conducting a vector similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa46e289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé [Query]: What is an embedding?\n",
      "\n",
      "üìÑ [Document 1]\n",
      "üìñ Document Content: Embedding\n",
      "üóÇÔ∏è Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n",
      "üìÑ [Document 2]\n",
      "üìñ Document Content: Definition: Embedding is the process of converting text data, such as words or sentences, into continuous low-dimensional vectors. This allows computers to understand and process text.\n",
      "Example: The word \"apple\" can be represented as a vector like [0.65, -0.23, 0.17].\n",
      "Related Keywords: Natural Language Processing, Vectorization, Deep Learning\n",
      "üóÇÔ∏è Metadata: {'source': './data/01-vectorstore-retriever-appendix-keywords.txt'}\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ 3. Query Embedding and Vector Search\n",
    "query_embedder = UpstageEmbeddings(model=\"solar-embedding-1-large-query\")\n",
    "\n",
    "# Convert the query into a vector using the query embedding model\n",
    "query_vector = query_embedder.embed_query(\"What is an embedding?\")\n",
    "\n",
    "# ‚úÖ 4. Vector Similarity Search (Return Top 2 Documents)\n",
    "results = db.similarity_search_by_vector(query_vector, k=2)\n",
    "\n",
    "# ‚úÖ 5. Display the Search Results\n",
    "print(f\"\\nüîé [Query]: What is an embedding?\\n\")\n",
    "for idx, doc in enumerate(results):\n",
    "    print(f\"üìÑ [Document {idx + 1}]\")\n",
    "    print(\"üìñ Document Content:\", doc.page_content)\n",
    "    print(\"üóÇÔ∏è Metadata:\", doc.metadata)\n",
    "    print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-opentutorial-bMU5IxA3-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
