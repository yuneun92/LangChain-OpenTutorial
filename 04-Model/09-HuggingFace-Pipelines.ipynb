{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "635d8ebb",
      "metadata": {},
      "source": [
        "# HuggingFace Pipeline\n",
        "\n",
        "- Author: [Sunworl Kim](https://github.com/sunworl)\n",
        "- Design: \n",
        "- Peer Review: [effort-type](https://github.com/effort-type), [sunworl](https://github.com/sunworl), [ivybae](https://github.com/ivybae)\n",
        "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/04-Model/08-HuggingFace-Pipelines.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/04-Model/08-HuggingFace-Pipelines.ipynb)\n",
        "\n",
        "\n",
        "## Overview\n",
        "\n",
        "This tutorial covers how to run Hugging Face models locally through the **HuggingFacePipeline** class.\n",
        "\n",
        "It explains how to load a model by specifying model parameters using the **from_model_id** method or by directly passing the **transformers pipeline**.\n",
        "\n",
        "Using the generated **hf** object, it implements text generation for a given prompt.\n",
        "\n",
        "By specifying parameters for the device, it also implements execution on a GPU device and batching.\n",
        "\n",
        "- **Advantages**  \n",
        "    - No usage fees.  \n",
        "    - Lower risk of data leakage.  \n",
        "\n",
        "- **Disadvantages**  \n",
        "    - Requires significant computational resources.   \n",
        "\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "- [Overview](#overview)\n",
        "- [Environment Setup](#environment-setup)\n",
        "- [Hugging Face Local pipelines](#hugging-face-local-pipelines)\n",
        "- [Model Loading](#model-loading)\n",
        "- [Usage of Gated Model](#usage-of-gated-model)\n",
        "- [Create Chain](#create-chain)\n",
        "- [GPU Inference](#gpu-inference)\n",
        "- [Batch GPU Inference](#batch-gpu-inference) \n",
        "\n",
        "\n",
        "### References\n",
        "\n",
        "- [Langchain: Hugging Face Local Pipelines](https://python.langchain.com/docs/integrations/llms/huggingface_pipelines/)\n",
        "- [Hugging Face: Phi-3-mini-4k-instuct](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) \n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "041c982e",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
        "\n",
        "**[Note]**\n",
        "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
        "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "47dc55d5",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%%capture --no-stderr\n",
        "!pip install langchain-opentutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f45a316c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "from langchain_opentutorial import package\n",
        "\n",
        "package.install(\n",
        "    [\n",
        "        \"langsmith\",    \n",
        "        \"langchain_core\",\n",
        "        \"langchain.prompts\",\n",
        "        \"langchain_huggingface\",\n",
        "        \"huggingface_hub\"\n",
        "    ],\n",
        "    verbose=False,\n",
        "    upgrade=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "0f2380ec",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment variables have been set successfully.\n"
          ]
        }
      ],
      "source": [
        "# Set environment variables\n",
        "from dotenv import load_dotenv\n",
        "from langchain_opentutorial import set_env\n",
        "\n",
        "# Attempt to load environment variables from a .env file; if unsuccessful, set them manually.\n",
        "if not load_dotenv():\n",
        "    set_env(\n",
        "        {\n",
        "            \"HUGGINGFACEHUB_API_TOKEN\": \"\",\n",
        "            \"LANGCHAIN_API_KEY\": \"\",\n",
        "            \"LANGCHAIN_TRACING_V2\": \"true\",\n",
        "            \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
        "            \"LANGCHAIN_PROJECT\": \"Huggingface-Piplines\",\n",
        "        }\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c860e8a8",
      "metadata": {},
      "source": [
        "## Hugging Face Local Pipelines\n",
        "\n",
        "The Hugging Face models can be run locally through the `HuggingFacePipeline` class.\n",
        "\n",
        "The [Hugging Face model Hub](https://huggingface.co/models) hosts over 120k models, 20k datasets, and 50k demo apps (Spaces) on its online platform, all of which are open-source and publicly available, allowing people to easily collaborate and build ML together.\n",
        "\n",
        "These can be used in LangChain either by calling them through this local pipeline wrapper or by calling hosted inference endpoints through the HuggingFaseHub class. For more information on hosted pipelines, please refer to the [HuggingFaseHub](https://huggingface.co/models) notebook.\n",
        "\n",
        "To use this, you should have the [transformers python package](https://pypi.org/project/transformers/) installed, as well as [PyTorch](https://pytorch.org/get-started/locally/).\n",
        "\n",
        "Additionally, you may install `xformers` for a more memory-efficient attention implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "21943adb",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU transformers\n",
        "!pip install -qU ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c1e4a1c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# installation pytorch\n",
        "# !pip install --force-reinstall --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu121"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "910b89cb",
      "metadata": {},
      "source": [
        "Set the path to download the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5ad1056d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path to download Hugging Face models/tokenizers\n",
        "import os\n",
        "\n",
        "# ./cache/ Set to download to the specified path\n",
        "os.environ[\"HF_HOME\"] = \"./cache/\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6c7aba4",
      "metadata": {},
      "source": [
        "## Model Loading\n",
        "\n",
        "Models can be loaded by specifying model parameters using the method `from_model_id`.\n",
        "\n",
        "\n",
        "- The `langchain-opentutorial` class is used to load a pre-trained model from Hugging Face.\n",
        "\n",
        "- The `from_model_id` method is used to specify the `microsoft/Phi-3-mini-4k-instruct` model and set the task to \"text-generation\".\n",
        "\n",
        "- The `pipeline_kwargs` parameter is used to limit the maximum number of tokens to be generated to 64.\n",
        "\n",
        "- The loaded model is assigned to the `hf` variable, which can be used to perform text generation tasks.\n",
        "\n",
        "The model used: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f25ec196",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b90913ad7d244cd6b35376669833c430",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "# Download the HuggingFace model.\n",
        "hf = HuggingFacePipeline.from_model_id(   \n",
        "    model_id=\"microsoft/Phi-3-mini-4k-instruct\",  # Specify the ID of the model to use.  \n",
        "    task=\"text-generation\",  # Specify the task to perform. Here, it's text generation.        \n",
        "    pipeline_kwargs={\"max_new_tokens\": 64},  # Set additional arguments to pass to the pipeline. Here, we limit the maximum number of new tokens to 64.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "690a9ae0",
      "metadata": {},
      "source": [
        "You can also load by directly passing an existing `transformers` pipeline.\n",
        "\n",
        "The text ageneration model is implemented using HuggingFacePipeline.\n",
        "\n",
        "\n",
        "- `AutoTokenizer` and `AutoModelForCausalLM` are used to load the `microsoft/Phi-3-mini-4k-instruct` model and tokenizer.\n",
        "\n",
        "- The `pipeline` function is used to create a \"text-generation\" pipeline, setting up the model and tokenizer. The maximum number of generated tokens is limited to 64.\n",
        "\n",
        "- The `HuggingFacePipeline` class is used to create an `hf` object, and the generated pipeline is passed to it.\n",
        "\n",
        "\n",
        "Using this created `hf` object, you can perform text generation for a given prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "4f99b5b6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e69973e9adfb439da9af291870ed6b80",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# Specify the ID of the model to use.\n",
        "model_id = \"microsoft/Phi-3-mini-4k-instruct\" \n",
        "# Load the tokenizer for the specified model. \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id) \n",
        "\n",
        "# Load the specified model.\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)  \n",
        "\n",
        "# Create a text generation pipeline and set the maximum number of new tokens to be generated to 64.\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=64)\n",
        "\n",
        "# Create a HuggingFacePipeline object and pass the generated pipeline to it.\n",
        "hf = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34cac03a",
      "metadata": {},
      "source": [
        "## Usage of Gated Model\n",
        "\n",
        "The `Gated Model` is a model that can be used under a license agreement from Hugging Face.\n",
        "\n",
        "You must first visit the model page and agree to the terms before obtaining a Hugging Face token.\n",
        "\n",
        "Below is an example of how to use the `Gated` Model. You need to specify the Hugging Face token as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "12f83216",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb6f98b1f46b4b5f84e7593969932b15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# Specify the model ID registered in the Hugging Face repository.\n",
        "model_id = \"microsoft/Phi-3-mini-4k-instruct\" \n",
        "\n",
        "# Enter the Hugging Face token you received here.\n",
        "your_huggingface_token = \"\"\n",
        "\n",
        "# Load the tokenizer.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=your_huggingface_token)\n",
        "\n",
        "# Load the specified model.\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    token=your_huggingface_token,\n",
        "    # load_in_4bit=True, # If bitsandbytes is installed (Linux)\n",
        "    # attn_implementation=\"flash_attention_2\", # If you have an Ampere GPU\n",
        ")\n",
        "\n",
        "# Create the pipeline.\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=64)\n",
        "\n",
        "# Create a HuggingFacePipeline object and pass the created pipeline.\n",
        "hf_llm = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1b6fd91",
      "metadata": {},
      "source": [
        "Execute and check the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d8a71215",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "# Answer\n",
            "The capital of France is Paris."
          ]
        }
      ],
      "source": [
        "for token in hf_llm.stream(\"What is the capital of France?\"):\n",
        "    print(token, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa00c3f4",
      "metadata": {},
      "source": [
        "## Create Chain\n",
        "\n",
        "Once the model is loaded into memory, you can configure it with prompts to form a chain.\n",
        "\n",
        "\n",
        "- A prompt template defining the question and answer format is created using the `PromptTemplate` class.\n",
        "\n",
        "- Create a `chain` object by connecting the `prompt` object and the `hf` object in a pipeline.\n",
        "\n",
        "- Call the `chain.invoke()` method to generate and output an answer for the given question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c00304bf",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26101335a2004c83bef3697d2f07b837",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "58e95a41",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_huggingface import ChatHuggingFace\n",
        "\n",
        "llm = ChatHuggingFace(llm=hf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "69cb77da",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|user|>\n",
            "<|system|>You are a helpful assistant.<|end|>\n",
            "<|user|>What is the capital of the United France?<|end|>\n",
            "<|assistant|><|end|>\n",
            "<|assistant|>\n",
            " The capital of France is Paris.\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "template = \"\"\"<|system|>You are a helpful assistant.<|end|>\n",
        "<|user|>{question}<|end|>\n",
        "<|assistant|>\"\"\"  # Template for defining question and answer formats\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)  # Create a prompt object using the template\n",
        "\n",
        "# Create a chain by connecting the prompt and the language model\n",
        "chain = prompt |llm| StrOutputParser()\n",
        "question = \"What is the capital of the United France?\"  # Define the question\n",
        "print(\n",
        "    chain.invoke({\"question\": question})\n",
        ")  # Call the chain to generate and output an answer to the question\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b2fc536",
      "metadata": {},
      "source": [
        "## GPU Inference\n",
        "\n",
        "When running on a GPU, you can specify the `device=n` parameter to place the model on a specific device.\n",
        "\n",
        "The default value is `-1`, which means inference is performed on the CPU.\n",
        "\n",
        "If you have multiple GPUs or if the model is too large for a single GPU, you can specify `device_map=\"auto\"`.\n",
        "\n",
        "In this case, the [Accelerate](https://huggingface.co/docs/accelerate/index) library is required and is used to automatically determine how to load the model weights.\n",
        "\n",
        "*Caution*: `device` and `device_map` should not be specified together, as this can cause unexpected behavior.\n",
        "\n",
        "\n",
        "\n",
        "- Load the `gpt2` model using `HuggingFacePipeline` and set the `device` parameter to 0 to run it on the GPU.\n",
        "\n",
        "- Limit the maximum number of tokens to be generated to 64 using the `pipeline_kwargs` parameter.\n",
        "\n",
        "- Connect the `prompt` and `gpu_llm` in a pipeline to create the `gpu_chain`.\n",
        "\n",
        "- Call the `gpu_chain.invoke()` method to generate and output an answer for the given question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "1b78d33f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d59e808676124512bc5bab734e268457",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|system|>You are a helpful assistant.<|end|>\n",
            "<|user|>What is the capital of France?<|end|>\n",
            "<|assistant|> The capital of France is Paris. It is not only the largest city in France but also one of the most important cultural and commercial centers in Europe. Paris is known for its historical landmarks such as the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum, which is the world\n"
          ]
        }
      ],
      "source": [
        "gpu_llm = HuggingFacePipeline.from_model_id(\n",
        "    \n",
        "    model_id=\"microsoft/Phi-3-mini-4k-instruct\", \n",
        "    task=\"text-generation\",      \n",
        "    device=-1,    # Specifies the GPU device number. -1 stands for CPU.   \n",
        "    pipeline_kwargs={\"max_new_tokens\": 64},  # Set additional arguments to be passed to the pipeline. In this case, limit the maximum number of tokens to be generated to 64.\n",
        ")\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)  # Create a prompt object using the template\n",
        "\n",
        "# Create a chain by connecting the prompt and the language model.\n",
        "gpu_chain = prompt | gpu_llm | StrOutputParser()\n",
        "\n",
        "question = \"What is the capital of France?\" \n",
        "\n",
        "#Invoke the chain to generate and output the answer to the question\n",
        "print(gpu_chain.invoke({\"question\": question}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9e4d831",
      "metadata": {},
      "source": [
        "## Batch GPU Inference\n",
        "\n",
        "When running on a GPU device, you can perform inference in batch mode on the GPU.\n",
        "\n",
        "\n",
        "- Load the `microsoft/Phi-3-mini-4k-instruct` model using `HuggingFacePipeline` and set it to run on the GPU.\n",
        "\n",
        "- When creating the `gpu_llm`, set the `batch_size` to 2, `temperature` to 0, and `max_length` to 64.\n",
        "\n",
        "- Connect the `prompt` and `gpu_llm` in a pipeline to create the `gpu_chain`, and set the end token to \"\\n\\n\".\n",
        "\n",
        "- Use `gpu_chain.batch()` to generate answers in parallel for the `questions` in the questions.\n",
        "\n",
        "- Wrap each answer with <answer> tags and separate each answer with a line break."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0874c14b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "040ab4f998bf4f1fab868c02bf7329a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|system|>You are a helpful assistant.<|end|>\n",
            "<|user|>What is the number 0 in English?<|end|>\n",
            "<|assistant|> The number 0 in English is called \"zero.\"\n",
            "\n",
            "<|system|>You are a helpful assistant.<|end|>\n",
            "<|user|>What is the number 1 in English?<|end|>\n",
            "<|assistant|> The number 1 in English is simply called \"one.\"\n",
            "\n",
            "<|system|>You are a helpful assistant.<|end|>\n",
            "<|user|>What is the number 2 in English?<|end|>\n",
            "<|assistant|> The number 2 in English is spelled \"two.\"\n",
            "\n",
            "<|system|>You are a helpful assistant.<|end|>\n",
            "<|user|>What is the number 3 in English?<|end|>\n",
            "<|assistant|> The number 3 in English is spelled \"three.\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "gpu_llm = HuggingFacePipeline.from_model_id(\n",
        "\n",
        "    model_id=\"microsoft/Phi-3-mini-4k-instruct\", \n",
        "    task=\"text-generation\",    \n",
        "    device=-1, # Specifies the GPU device number. -1 stands for CPU.       \n",
        "    batch_size=2,  # Adjust the batch size. Set it appropriately based on GPU memory and model size.\n",
        "    model_kwargs={\n",
        "        \"temperature\": 0,\n",
        "        \"max_length\": 64,\n",
        "        \"do_sample\": True\n",
        "    },  # Set additional arguments to be passed to the model.\n",
        ")\n",
        "\n",
        "# Create a chain by connecting the prompt and the language model.\n",
        "gpu_chain = prompt | gpu_llm.bind(stop=[\"\\n\\n\"]) \n",
        "\n",
        "questions = []\n",
        "for i in range(4):\n",
        "    # Generate a list of questions\n",
        "    questions.append({\"question\": f\"What is the number {i} in English?\"})\n",
        "\n",
        "answers = gpu_chain.batch(questions) \n",
        "for answer in answers:\n",
        "    print(answer)\n",
        "    print(\"\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "3.11.9",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
