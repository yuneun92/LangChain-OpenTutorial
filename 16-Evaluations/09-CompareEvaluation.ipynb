{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "635d8ebb",
      "metadata": {},
      "source": [
        "# Compare experiment evaluations \n",
        "\n",
        "- Author: [Yejin Park](https://github.com/ppakyeah)\n",
        "- Peer Review: \n",
        "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/99-TEMPLATE/00-BASE-TEMPLATE-EXAMPLE.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/99-TEMPLATE/00-BASE-TEMPLATE-EXAMPLE.ipynb)\n",
        "\n",
        "## Overview\n",
        "\n",
        "This tutorial demonstrates the process of backtesting and comparing model evaluations using LangSmith, focusing on assessing RAG system performance between GPT-4 and Ollama models.\n",
        "\n",
        "Through practical examples, you'll learn how to create evaluation datasets from production data, implement evaluation metrics, and analyze results using LangSmith's comparison features. \n",
        "\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "- [Overview](#overview)\n",
        "- [Environment Setup](#environment-setup)\n",
        "- [Backtesting with LangSmith](#backtesting-with-langsmith)\n",
        "\n",
        "### References\n",
        "\n",
        "- [LangSmith: Backtesting](https://docs.smith.langchain.com/evaluation/tutorials/backtesting)\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6c7aba4",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Setting up your environment is the first step. See the [Environment Setup](https://wikidocs.net/257836) guide for more details.\n",
        "\n",
        "\n",
        "**[Note]**\n",
        "\n",
        "The langchain-opentutorial is a package of easy-to-use environment setup guidance, useful functions and utilities for tutorials.\n",
        "Check out the  [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "21943adb",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain-opentutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f25ec196",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "from langchain_opentutorial import package\n",
        "\n",
        "package.install(\n",
        "    [\n",
        "        \"langsmith\",\n",
        "        \"langchain_ollama\",\n",
        "        \"langchain_openai\",\n",
        "        \"langchain_community\",\n",
        "        \"langchain_text_splitters\",\n",
        "        \"langchain_core\",\n",
        "        \"pymupdf\",\n",
        "        \"faiss-cpu\"\n",
        "    ],\n",
        "    verbose=False,\n",
        "    upgrade=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "690a9ae0",
      "metadata": {},
      "source": [
        "You can set API keys in a `.env` file or set them manually.\n",
        "\n",
        "**[Note]** If you’re not using the `.env` file, no worries! Just enter the keys directly in the cell below, and you’re good to go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "327c2c7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "from langchain_opentutorial import set_env\n",
        "\n",
        "# Attempt to load environment variables from a .env file; if unsuccessful, set them manually.\n",
        "if not load_dotenv():\n",
        "    set_env(\n",
        "        {\n",
        "            \"OPENAI_API_KEY\": \"\",\n",
        "            \"LANGCHAIN_API_KEY\": \"\",\n",
        "            \"LANGCHAIN_TRACING_V2\": \"true\",\n",
        "            \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
        "            \"LANGCHAIN_PROJECT\": \"09-CompareEvaluation\",\n",
        "        }\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa00c3f4",
      "metadata": {},
      "source": [
        "## Backtesting with LangSmith\n",
        "Backtesting involves assessing new versions of your application using historical data and comparing the new outputs to the original ones.\n",
        "\n",
        "Compared to evaluations using pre-production datasets, backtesting offers a clearer indication of whether the new version of your application is an improvement over the current deployment.\n",
        "\n",
        "Here are the basic steps for backtesting:\n",
        "\n",
        "1. Select sample runs from your production tracing project to test against.\n",
        "2. Transform the run inputs into a dataset and record the run outputs as an initial experiment against that dataset.\n",
        "3. Execute your new system on the new dataset and compare the results of the experiments.\n",
        "\n",
        "You can easily compare the results of your experiments by utilizing the Compare feature provided by LangSmith."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb96b63c",
      "metadata": {},
      "source": [
        "### Define Functions for RAG Performance Testing\n",
        "\n",
        "Let's create a RAG system to utilize for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9ef95e58",
      "metadata": {},
      "outputs": [],
      "source": [
        "from myrag import PDFRAG\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "# Function to answer questions\n",
        "def ask_question_with_llm(llm):\n",
        "    # Create PDFRAG object\n",
        "    rag = PDFRAG(\n",
        "        \"data/Newwhitepaper_Agents2.pdf\",\n",
        "        llm,\n",
        "    )\n",
        "\n",
        "    # Create retriever\n",
        "    retriever = rag.create_retriever()\n",
        "\n",
        "    # Create chain\n",
        "    rag_chain = rag.create_chain(retriever)\n",
        "\n",
        "    def _ask_question(inputs: dict):\n",
        "        context = retriever.invoke(inputs[\"question\"])\n",
        "        context = \"\\n\".join([doc.page_content for doc in context])\n",
        "        return {\n",
        "            \"question\": inputs[\"question\"],\n",
        "            \"context\": context,\n",
        "            \"answer\": rag_chain.invoke(inputs[\"question\"]),\n",
        "        }\n",
        "\n",
        "    return _ask_question"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9aba350c",
      "metadata": {},
      "source": [
        "This tutorial uses the `llama3.2` 1b model. Please make sure you have Ollama installed.\n",
        "\n",
        "For detailed information about Ollama, refer to the [GitHub tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/04-Model/10-Ollama.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1d8f7125",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Hello. Is there something I can help you with or would you like to chat?', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2025-01-15T06:23:09.277041Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1404527875, 'load_duration': 566634125, 'prompt_eval_count': 27, 'prompt_eval_duration': 707000000, 'eval_count': 18, 'eval_duration': 127000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-01a8d197-dc3a-4471-855a-36daac538e8b-0', usage_metadata={'input_tokens': 27, 'output_tokens': 18, 'total_tokens': 45})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "ollama = ChatOllama(model=\"llama3.2:1b\")\n",
        "\n",
        "# Call Ollama model\n",
        "ollama.invoke(\"Hello?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6f173f4",
      "metadata": {},
      "source": [
        "Create a function that utilizes the GPT-4o-mini model and the Ollama model to generate answers to your questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "aa9275c4",
      "metadata": {},
      "outputs": [],
      "source": [
        "gpt_chain = ask_question_with_llm(ChatOpenAI(model=\"gpt-4o-mini\", temperature=0))\n",
        "ollama_chain = ask_question_with_llm(ChatOllama(model=\"llama3.2:1b\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "875631bb",
      "metadata": {},
      "source": [
        "Then, evaluate the answers using the GPT-4o-mini model and the Ollama model.\n",
        "\n",
        "Do this for each of the two chains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2b91b764",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'MODEL_COMPARE_EVAL-05b6496b' at:\n",
            "https://smith.langchain.com/o/9089d1d3-e786-4000-8468-66153f05444b/datasets/9b4ca107-33fe-4c71-bb7f-488272d895a3/compare?selectedSessions=33fa8084-b82f-45ee-a3dd-c374caad16e0\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a415bebb3bfd4dcea36e1cf9b9fc49d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'MODEL_COMPARE_EVAL-c264adb7' at:\n",
            "https://smith.langchain.com/o/9089d1d3-e786-4000-8468-66153f05444b/datasets/9b4ca107-33fe-4c71-bb7f-488272d895a3/compare?selectedSessions=f784a8c4-88ab-4a35-89a7-3aba5367f182\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dad2cadac8f841fabc0af6edc32b34ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langsmith.evaluation import evaluate, LangChainStringEvaluator\n",
        "\n",
        "# Create QA evaluator\n",
        "cot_qa_evalulator = LangChainStringEvaluator(\n",
        "    \"cot_qa\",\n",
        "    config={\"llm\": ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)},\n",
        "    prepare_data=lambda run, example: {\n",
        "        \"prediction\": run.outputs[\"answer\"],\n",
        "        \"reference\": run.outputs[\"context\"],\n",
        "        \"input\": example.inputs[\"question\"],\n",
        "    },\n",
        ")\n",
        "\n",
        "dataset_name = \"RAG_EVAL_DATASET\"\n",
        "\n",
        "# Run gpt evaluation\n",
        "experiment_results1 = evaluate(\n",
        "    gpt_chain,\n",
        "    data=dataset_name,\n",
        "    evaluators=[cot_qa_evalulator],\n",
        "    experiment_prefix=\"MODEL_COMPARE_EVAL\",\n",
        "    metadata={\n",
        "        \"variant\": \"GPT-4o-mini Evaluation (cot_qa)\",\n",
        "    },\n",
        ")\n",
        "\n",
        "# Run ollama evaluation\n",
        "experiment_results2 = evaluate(\n",
        "    ollama_chain,\n",
        "    data=dataset_name,\n",
        "    evaluators=[cot_qa_evalulator],\n",
        "    experiment_prefix=\"MODEL_COMPARE_EVAL\",\n",
        "    metadata={\n",
        "        \"variant\": \"Ollama(llama3.2:1b) Evaluation (cot_qa)\",\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1acf3b1e",
      "metadata": {},
      "source": [
        "### Comparing the result\n",
        "\n",
        "Use the Compare view to inspect the results.\n",
        "\n",
        "1. In the Experiment tab of the dataset, select the experiment you want to compare.\n",
        "2. Click the “Compare” button at the bottom.\n",
        "3. The comparison view is displayed.\n",
        "\n",
        "![compare-view-01.png](./assets/09-compare-evaluation-compare-view-01.png)\n",
        "![compare-view-02png](./assets/09-compare-evaluation-compare-view-02.png)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain-opentutorial-0qAVdsEJ-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
