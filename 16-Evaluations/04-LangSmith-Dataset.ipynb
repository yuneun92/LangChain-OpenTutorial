{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "635d8ebb",
      "metadata": {},
      "source": [
        "# LangSmith-Dataset \n",
        "\n",
        "- Author: [Minji](https://github.com/r14minji)\n",
        "- Design: \n",
        "- Peer Review: \n",
        "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/99-TEMPLATE/00-BASE-TEMPLATE-EXAMPLE.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/99-TEMPLATE/00-BASE-TEMPLATE-EXAMPLE.ipynb)\n",
        "\n",
        "## Overview\n",
        "\n",
        "The notebook demonstrates how to create a dataset for evaluating Retrieval-Augmented Generation (RAG) models using LangSmith. It includes steps for setting up environment variables, creating datasets with questions and answers, and uploading examples to LangSmith for testing. Additionally, it provides instructions on using HuggingFace datasets and updating datasets with new examples.\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "- [Overview](#overview)\n",
        "- [Environment Setup](#environment-setup)\n",
        "- [Creating a LangSmith Dataset](#creating-a-LangSmith-Dataset)\n",
        "- [Creating a Dataset for LangSmith Testing](#creating-a-Dataset-for-LangSmith-Testing)\n",
        "\n",
        "\n",
        "### References\n",
        "\n",
        "- [LangChain](https://blog.langchain.dev/)\n",
        "- [LangSmith](https://docs.smith.langchain.com)\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6c7aba4",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Setting up your environment is the first step. See the [Environment Setup](https://wikidocs.net/257836) guide for more details.\n",
        "\n",
        "\n",
        "**[Note]**\n",
        "\n",
        "The langchain-opentutorial is a package of easy-to-use environment setup guidance, useful functions and utilities for tutorials.\n",
        "Check out the  [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "21943adb",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain-opentutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f25ec196",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "from langchain_opentutorial import package\n",
        "\n",
        "package.install(\n",
        "    [\n",
        "        \"langsmith\",\n",
        "        \"langchain\",\n",
        "        \"langchain_core\",\n",
        "        \"langchain-anthropic\",\n",
        "        \"langchain_community\",\n",
        "        \"langchain_text_splitters\",\n",
        "        \"langchain_openai\",\n",
        "    ],\n",
        "    verbose=False,\n",
        "    upgrade=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "690a9ae0",
      "metadata": {},
      "source": [
        "You can set API keys in a `.env` file or set them manually.\n",
        "\n",
        "[Note] If you’re not using the `.env` file, no worries! Just enter the keys directly in the cell below, and you’re good to go."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "327c2c7c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "from langchain_opentutorial import set_env\n",
        "\n",
        "# Attempt to load environment variables from a .env file; if unsuccessful, set them manually.\n",
        "if not load_dotenv():\n",
        "    set_env(\n",
        "        {\n",
        "            # \"OPENAI_API_KEY\": \"\",\n",
        "            # \"LANGCHAIN_API_KEY\": \"\",\n",
        "            \"LANGCHAIN_TRACING_V2\": \"true\",\n",
        "            \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
        "            \"LANGCHAIN_PROJECT\": \"\",  # set the project name same as the title\n",
        "        }\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa00c3f4",
      "metadata": {},
      "source": [
        "## Creating a LangSmith Dataset\n",
        "\n",
        "Let's learn how to build a custom RAG evaluation dataset.\n",
        "\n",
        "To construct a dataset, you need to understand three main processes:\n",
        "\n",
        "Case: Evaluating whether the retrieval is relevant to the question\n",
        "\n",
        "> Question - Retrieval\n",
        "\n",
        "![](./assets/langsmith-dataset-01.png)\n",
        "\n",
        "Case: Evaluating whether the answer is relevant to the question\n",
        "\n",
        "> Question - Answer\n",
        "\n",
        "![](./assets/langsmith-dataset-02.png)\n",
        "\n",
        "Case: Checking if the answer is based on the retrieved documents (Hallucination Check)\n",
        "\n",
        "> Retrieval - Answer\n",
        "\n",
        "![](./assets/langsmith-dataset-03.png)\n",
        "\n",
        "Thus, you typically need `Question`, `Retrieval`, and `Answer` information. However, it is practically challenging to construct ground truth for `Retrieval`.\n",
        "\n",
        "\n",
        "If ground truth for `Retrieval` exists, you can save and use it all in your dataset. Otherwise, you can create and use a dataset with only `Question` and `Answer`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb68938f",
      "metadata": {},
      "source": [
        "## Creating a LangSmith Dataset\n",
        "\n",
        "Use `inputs` and `outputs` to create a dataset.\n",
        "\n",
        "The dataset consists of `questions` and `answers`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "62f970c2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the name of the generative AI created ...</td>\n",
              "      <td>The name of the generative AI created by Samsu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>On what date did U.S. President Biden issue an...</td>\n",
              "      <td>On October 30, 2023, U.S. President Biden issu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Please briefly describe Cohere's data provenan...</td>\n",
              "      <td>Cohere's data provenance explorer is a platfor...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0  What is the name of the generative AI created ...   \n",
              "1  On what date did U.S. President Biden issue an...   \n",
              "2  Please briefly describe Cohere's data provenan...   \n",
              "\n",
              "                                              answer  \n",
              "0  The name of the generative AI created by Samsu...  \n",
              "1  On October 30, 2023, U.S. President Biden issu...  \n",
              "2  Cohere's data provenance explorer is a platfor...  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# List of questions and answers\n",
        "inputs = [\n",
        "    \"What is the name of the generative AI created by Samsung Electronics?\",\n",
        "    \"On what date did U.S. President Biden issue an executive order ensuring safe and trustworthy AI development and usage?\",\n",
        "    \"Please briefly describe Cohere's data provenance explorer.\"\n",
        "]\n",
        "\n",
        "# List of corresponding answers\n",
        "outputs = [\n",
        "    \"The name of the generative AI created by Samsung Electronics is Samsung Gauss.\",\n",
        "    \"On October 30, 2023, U.S. President Biden issued an executive order.\",\n",
        "    \"Cohere's data provenance explorer is a platform that tracks the sources and licensing status of datasets used for training AI models, ensuring transparency. It collaborates with 12 organizations and provides source information for over 2,000 datasets, helping developers understand data composition and lineage.\",\n",
        "]\n",
        "\n",
        "# Create question-answer pairs\n",
        "qa_pairs = [{\"question\": q, \"answer\": a} for q, a in zip(inputs, outputs)]\n",
        "\n",
        "# Convert to a DataFrame\n",
        "df = pd.DataFrame(qa_pairs)\n",
        "\n",
        "# Display the DataFrame,\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40373119",
      "metadata": {},
      "source": [
        "Alternatively, you can use the Synthetic Dataset generated in a previous tutorial.\n",
        "\n",
        "The code below shows an example of using an uploaded HuggingFace Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "25c5f5a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install -qU datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9b082991",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'teddylee777' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Download dataset from HuggingFace Dataset using the repo_id\u001b[39;00m\n\u001b[1;32m      6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mteddylee777\u001b[49m\u001b[38;5;241m/\u001b[39mrag\u001b[38;5;241m-\u001b[39msynthetic\u001b[38;5;241m-\u001b[39mdataset,  \u001b[38;5;66;03m# Dataset name\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     token\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHhf_DuYlelOdzpJBkxQYAMEYPYTaJsGQIGOqpc\u001b[39m\u001b[38;5;124m\"\u001b[39m],  \u001b[38;5;66;03m# Required for private datasets\u001b[39;00m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# View dataset by split\u001b[39;00m\n\u001b[1;32m     12\u001b[0m huggingface_df \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkorean_v1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_pandas()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'teddylee777' is not defined"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset\n",
        "import os\n",
        "\n",
        "# Download dataset from HuggingFace Dataset using the repo_id\n",
        "dataset = load_dataset(\n",
        "    teddylee777/rag-synthetic-dataset,  # Dataset name\n",
        "    token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"],  # Required for private datasets\n",
        ")\n",
        "\n",
        "# View dataset by split\n",
        "huggingface_df = dataset[\"korean_v1\"].to_pandas()\n",
        "huggingface_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d38f536e",
      "metadata": {},
      "source": [
        "## Creating a Dataset for LangSmith Testing\n",
        "\n",
        "- Create a new dataset under `Datasets & Testing`.\n",
        "\n",
        "![](./assets/langsmith-dataset-04.png)\n",
        "\n",
        "You can also create a dataset directly using the LangSmith UI from a CSV file.\n",
        "\n",
        "For more details, refer to the documentation below:\n",
        "\n",
        "- [LangSmith UI Documentation](https://docs.smith.langchain.com/observability/how_to_guides/tracing/upload_files_with_traces)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "775c632b",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = \"RAG_EVAL_DATASET\"\n",
        "\n",
        "\n",
        "# Function to create a dataset\n",
        "def create_dataset(client, dataset_name, description=None):\n",
        "    for dataset in client.list_datasets():\n",
        "        if dataset.name == dataset_name:\n",
        "            return dataset\n",
        "\n",
        "    dataset = client.create_dataset(\n",
        "        dataset_name=dataset_name,\n",
        "        description=description,\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "\n",
        "# Create dataset\n",
        "dataset = create_dataset(client, dataset_name)\n",
        "\n",
        "# Add examples to the created dataset\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\": q} for q in df[\"question\"].tolist()],\n",
        "    outputs=[{\"answer\": a} for a in df[\"answer\"].tolist()],\n",
        "    dataset_id=dataset.id,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "030a7674",
      "metadata": {},
      "source": [
        "You can add examples to the dataset later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "fb9c0372",
      "metadata": {},
      "outputs": [],
      "source": [
        "# New list of questions\n",
        "new_questions = [\n",
        "    \"What is the name of the generative AI created by Samsung Electronics?\",\n",
        "    \"Is it true that Google invested $2 billion in Teddynote?\",\n",
        "]\n",
        "\n",
        "# 새로운 답변 목록\n",
        "new_answers = [\n",
        "    \"The name of the generative AI created by Samsung Electronics is Teddynote.\",\n",
        "    \"This is not true. Google agreed to invest up to $2 billion in Anthropic, starting with $500 million and planning to invest an additional $1.5 billion in the future.\",\n",
        "]\n",
        "\n",
        "# Verify the updated version in the UI\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\": q} for q in new_questions],\n",
        "    outputs=[{\"answer\": a} for a in new_answers],\n",
        "    dataset_id=dataset.id,\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain-opentutorial-LLYV5Gls-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
